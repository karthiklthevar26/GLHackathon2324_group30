{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3941e512",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1271192076.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_2505/1271192076.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    pip install pyspark numpy pandas matplotlib seaborn scipy flask\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#installing required libraries\n",
    "!pip install pyspark numpy pandas matplotlib seaborn scipy flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13454dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e675bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating spark session\n",
    "spark_session = SparkSession.builder.appName(\"HousingRegression\").getOrCreate()\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_sql_context = SQLContext(spark_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacbce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting variable for csv files\n",
    "TRAIN_INPUT = 'train.csv'\n",
    "TEST_INPUT = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating pandas dataframe\n",
    "pd_train = pd.read_csv(TRAIN_INPUT)\n",
    "pd_test = pd.read_csv(TEST_INPUT)\n",
    "#columns with null values\n",
    "na_cols = pd_train.columns[pd_train.isna().any()].tolist()\n",
    "print(na_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how SalePrice is distributed against normal theoretical quantiles\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "res = stats.probplot(pd_train['SalePrice'], plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc86ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting scatter plot\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(15,5))\n",
    "axes[0].set_xlim(0,10)\n",
    "\n",
    "sns.scatterplot(data=pd_train, ax=axes[0], x='OverallQual', y='SalePrice')\n",
    "axes[0].set_title('OverallQual vs SalePrice')\n",
    "sns.scatterplot(data=pd_train, ax=axes[1], x='GarageCars', y='SalePrice')\n",
    "axes[1].set_title('GarageCars vs SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting scatter plot\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(15,5))\n",
    "axes[0].set_xlim(0, 6000)\n",
    "\n",
    "sns.scatterplot(data=pd_train, ax=axes[0], x='GrLivArea', y='SalePrice')\n",
    "axes[0].set_title('GrLivArea vs SalePrice')\n",
    "sns.scatterplot(data=pd_train, ax=axes[1], x='GarageArea', y='SalePrice')\n",
    "axes[1].set_title('GarageArea vs SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633854a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting scatter plot\n",
    "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(15,5))\n",
    "axes[0].set_xlim(0, 6000)\n",
    "\n",
    "sns.scatterplot(data=pd_train, ax=axes[0], x='TotalBsmtSF', y='SalePrice')\n",
    "axes[0].set_title('TotalBsmtSF vs SalePrice')\n",
    "sns.scatterplot(data=pd_train, ax=axes[1], x='1stFlrSF', y='SalePrice')\n",
    "axes[1].set_title('1stFlrSF vs SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the total number of missing values in each column of your Pandas DataFrame pd_train and sorting them in descending order\n",
    "total = pd_train.isnull().sum().sort_values(ascending=False)\n",
    "#calculating the percentage of missing values in each column of your Pandas DataFrame pd_train and sorting them in descending order\n",
    "percent = (pd_train.isnull().sum()/pd_train.shape[0]).sort_values(ascending=False)\n",
    "#combining the total count and percentage of missing values for each column in your Pandas DataFrame pd_train into a new DataFrame named missing\n",
    "missing = pd.concat([total, percent], axis=1, keys=['Total', 'Perc_missing'])\n",
    "missing.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be98ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns from your Pandas DataFrame pd_train where the percentage of missing values exceeds a threshold of  15% and removes columns with a high percentage of missing values.\n",
    "pd_train = pd_train.drop(columns=(missing[missing['Perc_missing'] >= 0.15]).index)\n",
    "pd_train.head()\n",
    "pd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column in your Pandas DataFrame pd_train and pd_test  named 'New', and populating it with the product of the values in the 'OverallQual', 'GarageArea', and 'GrLivArea' columns.\n",
    "pd_train['New'] = pd_train['OverallQual'] * pd_train['GarageArea'] * pd_train['GrLivArea']\n",
    "pd_test['New'] = pd_test['OverallQual'] * pd_test['GarageArea'] * pd_test['GrLivArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list named train_cols containing the column names of your Pandas DataFrame pd_train. \n",
    "train_cols = list(pd_train.columns)\n",
    "train_cols.remove('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc603640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make test ds feature set same as in train ds\n",
    "\n",
    "pd_test = pd_test[train_cols]\n",
    "pd_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd5cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying columns in your Pandas DataFrame pd_test that contain missing values (NaN)\n",
    "pd_test.columns[pd_test.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling missing values in specific columns for both training and test datasets (pd_train and pd_test and fillingit with calue \"None\"\n",
    "\n",
    "for col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n",
    "    pd_train[col] = pd_train[col].fillna(\"None\")\n",
    "    pd_test[col] = pd_test[col].fillna(\"None\")\n",
    "    \n",
    "for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n",
    "    pd_train[col] = pd_train[col].fillna(\"None\")\n",
    "    pd_test[col] = pd_test[col].fillna(\"None\")\n",
    "    \n",
    "for col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n",
    "    pd_train[col] = pd_train[col].fillna(0)\n",
    "    pd_test[col] = pd_test[col].fillna(0)\n",
    "\n",
    "\n",
    "pd_train['MasVnrArea'] = pd_train['MasVnrArea'].fillna(0)\n",
    "pd_test['MasVnrArea'] = pd_test['MasVnrArea'].fillna(0)\n",
    "\n",
    "pd_train['Electrical'] = pd_train['Electrical'].fillna(pd_train['Electrical'].mode()[0])\n",
    "pd_test['Electrical'] = pd_test['Electrical'].fillna(pd_test['Electrical'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a5c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the maximum count of missing values across all columns in the dataframe\n",
    "print(pd_train.isnull().sum().max()) \n",
    "print(pd_test.isnull().sum().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51683cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling missing values in the below columns of your Pandas DataFrame pd_test with the mean value of that column\n",
    "pd_test['BsmtFinSF1'] = pd_test['BsmtFinSF1'].fillna(pd_test['BsmtFinSF1'].mean())\n",
    "pd_test['BsmtFinSF2'] = pd_test['BsmtFinSF2'].fillna(pd_test['BsmtFinSF2'].mean())\n",
    "pd_test['BsmtUnfSF'] = pd_test['BsmtUnfSF'].fillna(pd_test['BsmtUnfSF'].mean())\n",
    "pd_test['TotalBsmtSF'] = pd_test['TotalBsmtSF'].fillna(pd_test['TotalBsmtSF'].mean())\n",
    "pd_test['BsmtFullBath'] = pd_test['BsmtFullBath'].fillna(pd_test['BsmtFullBath'].mean())\n",
    "pd_test['BsmtHalfBath'] = pd_test['BsmtHalfBath'].fillna(pd_test['BsmtHalfBath'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d9b4f3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2505/2044398494.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#column names of categorical variables (columns with data type 'object')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcat_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpd_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_columns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NoData'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpd_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_columns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NoData'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd_train' is not defined"
     ]
    }
   ],
   "source": [
    "#column names of categorical variables (columns with data type 'object') \n",
    "cat_columns = pd_train.select_dtypes(include=['object']).columns\n",
    "pd_train[cat_columns] = pd_train[cat_columns].fillna('NoData')\n",
    "pd_test[cat_columns] = pd_test[cat_columns].fillna('NoData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbd32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping outliers from your  train dataset based on below conditions\n",
    "print(\"Dropping outliers resulted in %d instances in the new dataset\" % len(pd_train))\n",
    "pd_train = pd_train.drop(pd_train[(pd_train['GrLivArea']>4500) \n",
    "                                & (pd_train['SalePrice']<300000)].index)\n",
    "print(\"Dropping outliers resulted in %d instances in the new dataset\" % len(pd_train))\n",
    "pd_train = pd_train.drop(pd_train[(pd_train['GrLivArea']>5500) \n",
    "                                | (pd_train['SalePrice']>500000)].index)\n",
    "print(\"Dropping outliers resulted in %d instances in the new dataset\" % len(pd_train))\n",
    "pd_train = pd_train.drop(pd_train[pd_train['GarageArea']>1100].index)\n",
    "print(\"Dropping outliers resulted in %d instances in the new dataset\" % len(pd_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ce172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting your Pandas DataFrames to spark dataframes\n",
    "train_df = spark_session.createDataFrame(pd_train)\n",
    "test_df = spark_session.createDataFrame(pd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting columns in your Spark DataFrames (train_df and test_df) based on a list of columns that contain missing values (na_cols) and removing saleprice column from list of columns and selects only the columns listed in train_cols from the test_df DataFrame.\n",
    "\n",
    "train_df = train_df.select([c for c in train_df.columns if c not in na_cols])\n",
    "train_cols = train_df.columns\n",
    "train_cols.remove('SalePrice')\n",
    "test_df = test_df.select(train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cast below columns in your Spark DataFrame (test_df) to the IntegerType\n",
    "\n",
    "test_df = test_df.withColumn(\"BsmtFinSF1\", test_df[\"BsmtFinSF1\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"BsmtFinSF2\", test_df[\"BsmtFinSF2\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"BsmtUnfSF\", test_df[\"BsmtUnfSF\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"TotalBsmtSF\", test_df[\"TotalBsmtSF\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"BsmtFullBath\", test_df[\"BsmtFullBath\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"BsmtHalfBath\", test_df[\"BsmtHalfBath\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"GarageCars\", test_df[\"GarageCars\"].cast(IntegerType()))\n",
    "test_df = test_df.withColumn(\"GarageArea\", test_df[\"GarageArea\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1974e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining string columns to pass on to the String Indexer (= categorical feature encoding)\n",
    "#identifying columns with a data type of 'string' in Spark DataFrame train_df to list  train_string_columns\n",
    "train_string_columns = []\n",
    "\n",
    "for col, dtype in train_df.dtypes:\n",
    "    if dtype == 'string':\n",
    "        train_string_columns.append(col)\n",
    "print(train_string_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ecee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark's StringIndexer to convert categorical columns with string values into numerical indices in your Spark DataFrame train_df. \n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+'_index', handleInvalid='keep').fit(train_df) for column in train_string_columns]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "train_indexed = pipeline.fit(train_df).transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c948a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_indexed.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ea4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying columns with a data type of 'string' in Spark DataFrame test_df to list  test_string_columns\n",
    "test_string_columns = []\n",
    "for col, dtype in test_df.dtypes:\n",
    "    if dtype == 'string':\n",
    "        test_string_columns.append(col)\n",
    "print(test_string_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69eba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PySpark's StringIndexer to convert categorical columns with string values into numerical indices in your Spark DataFrame test_df. \n",
    "indexers2 = [StringIndexer(inputCol=column, outputCol=column+'_index', handleInvalid='keep').fit(test_df) for column in test_string_columns]\n",
    "pipeline2 = Pipeline(stages=indexers2)\n",
    "test_indexed = pipeline2.fit(test_df).transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaba995",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_indexed.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b32a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying and selecting numerical columns in your Spark DataFrames (train_indexed and test_indexed)  using get_dtype function\n",
    "\n",
    "def get_dtype(df,colname):\n",
    "    return [dtype for name, dtype in df.dtypes if name == colname][0]\n",
    "\n",
    "num_cols_train = []\n",
    "for col in train_indexed.columns:\n",
    "    if get_dtype(train_indexed,col) != 'string':\n",
    "        num_cols_train.append(str(col))\n",
    "        \n",
    "num_cols_test = []\n",
    "for col in test_indexed.columns:\n",
    "    if get_dtype(test_indexed,col) != 'string':\n",
    "        num_cols_test.append(str(col))\n",
    "\n",
    "train_indexed = train_indexed.select(num_cols_train)\n",
    "test_indexed = test_indexed.select(num_cols_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_indexed.columns))\n",
    "print(len(test_indexed.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PySpark's VectorAssembler to assemble the features into a single vector column features in Spark DataFrame train_indexed.\n",
    "vectorAssembler = VectorAssembler(inputCols = train_indexed.drop(\"SalePrice\").columns, outputCol = 'features').setHandleInvalid(\"keep\")\n",
    "train_vector = vectorAssembler.transform(train_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f229fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PySpark's VectorAssembler to assemble the features into a single vector column features  Spark DataFrame test_indexed.\n",
    "vectorAssembler2 = VectorAssembler(inputCols = test_indexed.columns, outputCol = 'features').setHandleInvalid(\"keep\")\n",
    "test_vector = vectorAssembler2.transform(test_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bda1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding a new column named \"SalePrice\" to your Spark DataFrame test_vector and populating it with a constant value of 0 using the lit function\n",
    "test_vector = test_vector.withColumn(\"SalePrice\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843e5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting  training data (train_vector) into training and validation sets using PySpark's randomSplit methood and are stored in the train and val DataFrames.\n",
    "\n",
    "splits = train_vector.randomSplit([0.7, 0.3])\n",
    "train = splits[0]\n",
    "val = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261b09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark's RandomForestRegressor to train a random forest model and evaluating it on the validation set.\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol = 'features', labelCol='SalePrice', \n",
    "                           maxDepth=20, \n",
    "                           minInstancesPerNode=2,\n",
    "                           bootstrap=True\n",
    "                          )\n",
    "#rf_model = rf.fit(train)\n",
    "\n",
    "# Create a Pipeline with your model\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# Fit the model\n",
    "rf_model = pipeline.fit(train)\n",
    "\n",
    "# Save the model\n",
    "rf_model.save(\"model/houseprice_model\")\n",
    "\n",
    "rf_predictions = rf_model.transform(val)\n",
    "rf_predictions.select(\"prediction\",\"SalePrice\",\"features\").show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "rf_evaluator = RegressionEvaluator(predictionCol=\"prediction\",                  labelCol=\"SalePrice\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on val data = %g\" % rf_evaluator.evaluate(rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e8657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using RF model to make predictions on the test dataset (test_vector) and preparing the results for submission\n",
    "rf_predictions2 = rf_model.transform(test_vector)\n",
    "#rf_predictions2.printSchema()\n",
    "pred = rf_predictions2.select(\"Id\",\"prediction\")\n",
    "pred = pred.withColumnRenamed(\"prediction\",\"SalePrice\")\n",
    "\n",
    "pred = pred.withColumn(\"Id\", pred[\"Id\"].cast(IntegerType()))\n",
    "pred = pred.withColumn(\"SalePrice\", pred[\"SalePrice\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the PySpark DataFrame pred to a Pandas DataFrame (pred_pd) and then saving it to a CSV file named \"submission.csv\".\n",
    "pred_pd = pred.toPandas()\n",
    "save = pred_pd.to_csv(\"submission.csv\", index=False)\n",
    "print(pred_pd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
